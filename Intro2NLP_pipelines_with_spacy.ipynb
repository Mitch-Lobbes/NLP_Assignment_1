{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "# Lab 1: Pipelines with spaCy\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "This notebook is based on an earlier version developed by Piek Vossen and Selene Baez. The [original version](https://github.com/cltl/ma-hlt-labs/blame/master/lab1.toolkits/Lab1.3-introduction-to-spaCy.ipynb) is more detailed and might be helpful if you have limited programming experience.\n",
    "\n",
    "[SpaCy](https://spacy.io/) combines multiple natural language processing analyses in a single Python package: it takes a raw document and can perform tokenization, POS-tagging, stop word recognition, morphological analysis, lemmatization, sentence splitting, dependency parsing and Named Entity Recognition (NER). The advantage of spaCy is that it is really fast, and it has a good accuracy. In addition, it currently supports multiple languages, among which: English, German, Spanish, Portuguese, French, Italian and Dutch. Other popular Python packages are [nltk](https://www.nltk.org/) and [stanza](https://stanfordnlp.github.io/stanza/).\n",
    "\n",
    "In this notebook, we will show you the basic usage of spaCy. Please additionally check the [user guides](https://spacy.io/usage/linguistic-features) and the documentation of the [models](https://spacy.io/models) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and loading spaCy\n",
    "\n",
    "To install spaCy, check out the instructions [here](https://spacy.io/usage). On this page, it is explained exactly how to install spaCy for your operating system, package manager and desired languages. Simply run the suggested commands in your terminal ([Anaconda Prompt](https://docs.anaconda.com/anaconda/user-guide/getting-started/) or cmd). In this notebook, we are going to download the English language resources. The standard download command from the command line is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first load spaCy in the notebook and check if we can load the English language resources. We import the spaCy module and load the English tokenizer, tagger, parser, NER, and word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# This loads a small English model trained on web data.\n",
    "# For other models and languages check: https://spacy.io/models\n",
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spaCy\n",
    "\n",
    "If you succesfully loaded the English model (or another language), you now created the spaCy object 'nlp'. You can use it to process text through a defined pipeline of modules and store the result as a value for another variable for accessing it. The results is another spaCy object of the type 'Doc' which gives you access to all the different analyses of the pipeline through different functions. In a Doc object you can access tokens, their lemmas, their PoS, sentences, chunks, named entities, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"Der Dativ ist dem Gentitiv sein Feind. Außerdem haben wir keinen Kafee mehr.\"\n",
    "# Let's run the NLP pipeline on our test input\n",
    "doc = nlp(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "The basic unit in NLP is usually the token. Let's examine how spaCy tokenizes the input. \n",
    "Note that punctuation is treated as a separate token and check how \"It's\" is tokenized. **Try a few other test inputs to better understand the concept of a token.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Der 0\n",
      "1 Dativ 4\n",
      "2 ist 10\n",
      "3 dem 14\n",
      "4 Gentitiv 18\n",
      "5 sein 27\n",
      "6 Feind 32\n",
      "7 . 37\n",
      "8 Außerdem 39\n",
      "9 haben 48\n",
      "10 wir 54\n",
      "11 keinen 58\n",
      "12 Kafee 65\n",
      "13 mehr 71\n",
      "14 . 75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.i, token, token.idx)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, spaCy provides sentence segmentation by grouping tokens together. **Try different test inputs to analyze the quality of the sentence segmentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Der Dativ ist dem Gentitiv sein Feind.\n",
      "Der\n",
      "Dativ\n",
      "ist\n",
      "dem\n",
      "Gentitiv\n",
      "sein\n",
      "Feind\n",
      ".\n",
      "\n",
      "Außerdem haben wir keinen Kafee mehr.\n",
      "Außerdem\n",
      "haben\n",
      "wir\n",
      "keinen\n",
      "Kafee\n",
      "mehr\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sentences = doc.sents\n",
    "for sentence in sentences:\n",
    "    print()\n",
    "    print(sentence)\n",
    "    for token in sentence:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lemmatization\n",
    "The Token object contains much more information than just the String representing the word. For example, you can access the lemma of each token. Note, that spaCy delivers a good accuracy, but it does make mistakes. **Make sure you understand the difference between a token and a lemma. Try out a few tricky cases as test input to analyze the quality of the lemmatizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der der\n",
      "Dativ Dativ\n",
      "ist sein\n",
      "dem der\n",
      "Gentitiv Gentitiv\n",
      "sein mein\n",
      "Feind Feind\n",
      ". .\n",
      "Außerdem Außerdem\n",
      "haben haben\n",
      "wir ich\n",
      "keinen kein\n",
      "Kafee Kafee\n",
      "mehr mehr\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. POS-Tagging\n",
    "A part-of-speech tagger assigns a word class to each token. The number of word classes depends on the tagset that the model uses. The most simplistic tags are the [universal POS-tags](https://universaldependencies.org/u/pos/all.html). Most models use more complex tagsets, but they also provide a mapping into the universal POS tags. SpaCy provides both: \n",
    "\n",
    "* the attribute **pos_** returns the universal part-of-speech tag\n",
    "* the attribute **tag_** provides a more finegrained tag\n",
    "\n",
    "The English model uses the [Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der DET ART\n",
      "Dativ NOUN NN\n",
      "ist AUX VAFIN\n",
      "dem DET ART\n",
      "Gentitiv NOUN ADJD\n",
      "sein DET PPOSAT\n",
      "Feind NOUN NN\n",
      ". PUNCT $.\n",
      "Außerdem ADV PROAV\n",
      "haben AUX VAFIN\n",
      "wir PRON PPER\n",
      "keinen DET PIAT\n",
      "Kafee NOUN NN\n",
      "mehr ADV ADV\n",
      ". PUNCT $.\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand the different tag labels. SpaCy provides a short explanation, but you also need to check the documentation and the reading material. **Find examples for words that can be assigned different POS-tags depending on the context.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finite verb, auxiliary'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"VAFIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Token` objects have many more useful methods and attributes. List them using the Python function `dir()`. You can find more detailed information about the token methods and attributes in the [documentation](https://spacy.io/api/token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_token = doc[0]\n",
    "dir(first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the attributes without `_` return numerical values which spaCy uses internally. Variants with `_` provide the human readable rendering of the value in unicode. **Explore some of the attributes and test them for different tokens.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12777781219550681880 FM\n"
     ]
    }
   ],
   "source": [
    "print(first_token.tag, first_token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Named Entity Recognition\n",
    "\"A named entity is a “real-world object” that is assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\" [[spaCy documentation]](https://spacy.io/usage/linguistic-features#named-entities)\n",
    "\n",
    "Explore the named entities in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apples neues iPhone ist in Deutschland auf dem Markt.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple's neues iPhone MISC\n",
      "Deutschland LOC\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Apples neues \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPhone\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " ist in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Deutschland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " auf dem Markt.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displacy provides nice visualizations of spaCy annotations https://spacy.io/usage/visualizers\n",
    "from spacy import displacy\n",
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The English model is trained on a dataset called *OntoNotes* (version 5). **How many different named entity types are annotated in this dataset? Have a look at the [documentation](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculating frequencies\n",
    "A common analysis step for language corpora is the extraction of frequency statistics. We provide an example to extract token frequencies, but you can also calculate frequencies over lemmas, n-grams, POS-labels, ...\n",
    "\n",
    "We calculate the statistics over a single input in this example. Usually, you would calculate them over all documents in a dataset. **How do you need to adjust the code to achieve this?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'to': 5, '’s': 4, 'raw': 3, 'text': 3, 'words': 3, 'it': 3, 'different': 3, 'in': 3, 'a': 3, 'is': 2, 'difficult': 2, 'and': 2, 'that': 2, 'completely': 2, 'mean': 2, 'the': 2, 'same': 2, 'can': 2, 'useful': 2, 'Processing': 1, 'intelligently': 1, 'most': 1, 'are': 1, 'rare': 1, 'common': 1, 'for': 1, 'look': 1, 'almost': 1, 'thing': 1, 'The': 1, 'order': 1, 'something': 1, 'Even': 1, 'splitting': 1, 'into': 1, 'word-like': 1, 'units': 1, 'be': 1, 'many': 1, 'languages': 1, 'While': 1, 'possible': 1, 'solve': 1, 'some': 1, 'problems': 1, 'starting': 1, 'from': 1, 'only': 1, 'characters': 1, 'usually': 1, 'better': 1, 'use': 1, 'linguistic': 1, 'knowledge': 1, 'add': 1, 'information': 1, 'That': 1, 'exactly': 1, 'what': 1, 'spaCy': 1, 'designed': 1, 'do': 1, 'you': 1, 'put': 1, 'get': 1, 'back': 1, 'Doc': 1, 'object': 1, 'comes': 1, 'with': 1, 'variety': 1, 'of': 1, 'annotations': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Our test input is the first paragraph of https://spacy.io/usage/linguistic-features\n",
    "test_input = \"Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it’s possible to solve some problems starting from only the raw characters, it’s usually better to use linguistic knowledge to add useful information. That’s exactly what spaCy is designed to do: you put in raw text, and get back a Doc object, that comes with a variety of annotations.\"\n",
    "# Let's run the NLP pipeline on our test input\n",
    "doc = nlp(test_input)\n",
    "\n",
    "word_frequencies = Counter()\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    words = []\n",
    "    for token in sentence: \n",
    "        # Let's filter out punctuation\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "    word_frequencies.update(words)\n",
    "    \n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 104 73\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "\n",
    "print(num_tokens, num_words, num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
